# datasets introduction
- **mySlideDataset.py**: 
- **myTileDataset.py**: data pool for image tiles, output is original image tiles and corresponding prior images


# model introduction
## Base model
- **model_v0**: pretrained ViT
    - add one base function in ViT class, for better calling specific function module
        - encoder(self,x)
- **model_v1**: Visual Prompt Tuning (ECCV'22, official implement based on vision_transformer.py)
- **model_v11**: Visual Query Tuning (VQT: CVPR'23, official implement based on vqt_vit.py)
- **model_v12**: VQT + Ours Quantitative prompts (custom implement based on vqt_vit_custom.py)
- **model_v13**: VQT + Ours Quantitative prompts (custom implement based on vision_transformer_custom.py)
    - append cls token behind prompt token (vs, always put cls token at first position)
- **model_v14**: Visual Query Tuning (VQT: CVPR'23, custom implement based on vision_transformer_custom.py)
    - append cls token behind prompt token (VS, always put cls token at first position)

- **model_v2**: Learn to Generate Prompt (arXiv'23)
## Ours (prompts and confounders p(y|do(x)))
- **Vision prompts based on prior image**
    - **G(prior):** generate prompts from prior segmentation image directly
        - **model_v3**: nuclei image prompt @ white image prompt
        - **model_v31**: apply prompt generator (encoder+decoder) to (priorSeg )
        - **model_v32**: original image prompt @ nuclei image prompt @ white image prompt
    - **G(Q(prior)):** learn prompts from quantitative attributes extracted from prior segmentation image
        - **model_v33**: project and generate prompts directly
        - **model_v34**: generate cooeficients and multify it with parameters embeddings (dictionary), resulting prompts
        - **model_v35**: dictionary + gate
        - <span style="background-color: red">**model_v36.py**</span>: using self attention (encoder+decoder) to generate prompts. (BEST)
        - **model_v361**: based on model_v36, enable calculate mean value of a distribution (requrest by reviewer)
        - **model_v37**: using self attention (encoder only) to generate prompts
        - **model_v39**: apply prompt generator (encoder+decoder) to (priorSeg + quantitative attributes)
        using matrix factorization to generate prompts

- **Vision-driven prompts generated by original image using visoin model (SAM)**
    - **model_v4** : SAM-based method:
    - **model_v41**: HistoML-based method: segmentation --> detection -> quantitative analysis

## Key libs
- **promptEncoder.py**: custom prompt generator
- **vision_transformer.py**: the lib to build VPT
- **vqt_vit.py**: the lib to build VQT
- **custom_vqt_vit.py**: develop based on 'vqt_vit.py', to implement VQT + quantitative attribtues
- **vision_transformer_custom.py** develop based on 'vision_transformer.py', to implement VQT + quantitative attributes


# Key function operators
## ~/utils/
    - **spatial_statistics.py**
        - calculate_segmentation_properties(): only return properties of max area region
        - calculate_segmentation_properties_histogram(): return 10 bins histogram
        - calculate_k_function(): return 140 length K(r)

    - **generate_props.py**
    - **analysis_props.py**



## ~/models/promptEncoders.py
    - promptSAT: using official transformerencoderlayer
    - promptEncoderDecoder: using decoder to generate prompts



## Extension (2024.09.01): positive and negative prompts + constrastive learning
- ssh script (run5*.sh)
- **promptEncoders.py**
    - QKVPosNegPromptG (+):  using positive and negative prompts to generate prompts
        - args(...,pos_promptLen, neg_promptLen, pos_promptLayers=12,neg_promptLayers=12)
- **params.py**
    - add pos_promptLen, neg_promptLen, pos_promptLayers, neg_promptLayers (+)
    - add pos_ortho_weights, neg_ortho_weights (+)
- **model_v5.py**: 
    - for positive prompts, append it to input image embedding tokens
    - for negative prompts, filter cls token through orthogonalization
- <span style="background-color: green">**model_v51.py**</span>: (only process cls token for pos and neg, 'QKVPosNegPromptG') (by 2024.10.15)
    - for positive prompts, enhance cls token through orthogonalization
    - for negative prompts, filter cls token through orthogonalization
    - Step-1: initialize pos and neg visual promtps $P^{+}_{0}$, $P^{-}_{0}$;
    - Step-2: Ortho($P^{+}_{0}$, $P^{-}_{0}$);
    - Step-3: $P^{+} = CrossAttention(P^{+}_{0}, V)$; $P^{-} = CrossAttention(P^{-}_{0}, V)$; (same CrossAttention)
    - Step-4: Proj($P^{+}$), Proj($P^{-}$); Projection again!
    - Step-5: applying orthogonalization on cls token one-after-one;
    - **model_v511.py**: based on **model_v51.py**,
        - Step-1 ~ Step-4 are same with **model_v51.py**
        - Step-5, applying orthogonalization on cls token at same time;
    - **model_v5121.py**: distinguish by different process module (cross-attention)
        - Given initialized base prompt embeddings $P^0$, visual attributes embeddings $A^0$
        - Step-1: $A^{+} = F^{+}(A^0)$; $A^{-} = F^{-}(A^0)$, where $F^{+}$ and $F^{-}$ are two independent functions and orthogonal to each other
        - Step-2: $P^{+} = CrossAttention^{+}(P^0, A^{+})$; $P^{-} = CrossAttention^{-}(P^0, A^{-})$; where $CrossAttention^{+}$ and $CrossAttention^{-}$ are two independent functions and their attention weights are orthogonal to each other
        - Step-3: $P^{+} = Proj^{+}(P^{+})$; $P^{-} = Proj^{-}(P^{-})$; Projection again!
    - **model_v5122.py**: based on **model_v511.py**
        - Step-1: same with **model_v511.py**
        - Step-2: torch.nn.init.orthogonal_($P_0^{+}$, $P_0^{-}$)
        - Step-3 ~ Step-5: same with **model_v511.py**
    - **model_v5123.py**: based on **model_v511.py**
        - add layer normalization on visual attributes embeddings $A^0$
    - **model_v5124.py**: based on **model_v511.py**
        - add inst normalization on visual attributes embeddings $A^0$

    - **model_v513.py**: orthogonalization on weights + gated function
        
    - **model_v514.py**: 
    - **model_v515.py**: negative visual prompts are **devired** from positive visual prompts, rather than initializing two sets of visual prompts
        - Rewrite **QKVOrthoPosNegPromptG** and **PosNegCrossAttention**
        - Step-1: initialize pos prompt embeddings 
        - Step-2: $P_{pos}，S_{pos} = CrossAttention(P_{pos}, V)$ (Return: positive visual prompts and attention scores)
        - Step-3: $S_{neg} = 1- S_{pos}$
        - Step-4: $P_{neg} = F(S_{neg},V)$
        - Step-5: Orthogonalization($P_{pos}$, $P_{neg}$) (Using Monica to verify the implementation code)
    - **model_v516.py**: based on **model_v515.py**, rewrite 
        - $V^{\prime}=V+w_i^+<P_{pos},V>-w_i^-<P_{neg},V>$ (using same original $v$ for subspace projection. Reduce the operation order difference between pos and neg prompts)
    - **model_v517.py**: transformer encoder-decoder based implementation 
        - Iitialize prompts $P$
        - TransformerEncoder： $V=f_{enc}(V)$
        - Mask generation: $M_{pos}, M_{neg} = f_{mask}(V,P)$ 
        - TransformerDecoder: $P_{pos}=f_{dec}(V,P,M_{pos})$
        - TransformerDecoder: $P_{neg}=f_{dec}(V,P,M_{neg})$
        - Orthogonalization: $P_{pos} \perp P_{neg}$
    - **model_v5171.py**: transformer encoder-decoder based implementation 
        - Iitialize pos prompts $P_{0}^{+}$ and neg prompts $P_{0}^{-}$.
        - Orthogonal pos and neg prompts $P_{neg} \perp P_{pos}$ 
        - TransformerEncoder： $A=f_{enc}(A_{0})$
        - TransformerDecoder: $P^{+}=f_{dec}(A,P_{0}^{+})$
        - TransformerDecoder: $P^{-}=f_{dec}(A,P_{0}^{-})$
    - **model_v5172.py**: transformer encoder + dual decoder based implementation 
        - Iitialize pos prompts $P_{0}^{+}$ and neg prompts $P_{0}^{-}$.
        - Orthogonal pos and neg prompts $P_{neg} \perp P_{pos}$ 
        - TransformerEncoder： $A=f_{enc}(A_{0})$
        - TransformerDecoder: $P^{+}=f_{dec}^{+}(A,P_{0}^{+})$
        - TransformerDecoder: $P^{-}=f_{dec}^{-}(A,P_{0}^{-})$
        - Projection: $P^{+}=f^{+}_{proj}(P^{+})$
        - Projection: $P^{-}=f^{-}_{proj}(P^{-})$
    - **model_v5173.py**: transformer decoder based implementation 
        - Iitialize pos prompts $P_{0}^{+}$ and neg prompts $P_{0}^{-}$.
        - Orthogonal pos and neg prompts $P_{neg} \perp P_{pos}$ 
        - TransformerDecoder: $P^{+}=f_{dec}(A,P_{0}^{+})$
        - TransformerDecoder: $P^{-}=f_{dec}(A,P_{0}^{-})$
    - **model_v5174.py**: transformer dual decoder based implementation 
        - Iitialize pos prompts $P_{0}^{+}$ and neg prompts $P_{0}^{-}$.
        - Orthogonal pos and neg prompts $P_{neg} \perp P_{pos}$ 
        - TransformerDecoder: $P^{+}=f_{dec}^{+}(A,P_{0}^{+})$
        - TransformerDecoder: $P^{-}=f_{dec}^{-}(A,P_{0}^{-})$
        - Projection: $P^{+}=f^{+}_{proj}(P^{+})$
        - Projection: $P^{-}=f^{-}_{proj}(P^{-})$
    -- **model_v5176.py**: transformer decoder based implementation + encoder-decoder based amplify and weaken
        - Iitialize pos prompts $P_{0}^{+}$ and neg prompts $P_{0}^{-}$.
        - Orthogonal pos and neg prompts $P_{neg} \perp P_{pos}$ 
        - TransformerDecoder: $P^{+}=f_{dec}(A,P_{0}^{+})$
        - TransformerDecoder: $P^{-}=f_{dec}(A,P_{0}^{-})$
        - Projection: $P^{+}=f_{proj}(P^{+})$
        - Projection: $P^{-}=f_{proj}(P^{-})$
        - x_first_token=EncoderDecoder(x_first_token,P^+,P^-)
    - **model_v518.py**: transformer encoder-decoder, with a global tagging mask indicating positive and negative
* **model_v52.py**: 
    - for positive prompts, enhance all image token through orthogonalization
    - for negative prompts, filter all image token through orthogonalization


## Extension rebuttal (2025.02.10)
- **model_v5126.py**: 
    - enable only positive prompting
- **model_v5127.py**: 
    - enable only using negative prompting

- **model_v5128.py**
    - enable only using human-defined features for classification

- **model_v15.py**: 
    - E2VPT
- **model_v16.py**:
    - VFPT
    



 